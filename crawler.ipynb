{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d26a42",
   "metadata": {},
   "source": [
    "# Import the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbc9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# File operations\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# PDF handling\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader  # PdfFileReader has been replaced by PdfReader in PyPDF2 2.x versions\n",
    "\n",
    "# JSON handling\n",
    "import json\n",
    "\n",
    "# OpenAI API\n",
    "import openai\n",
    "openai.api_key = \"<openai_api_key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ba145",
   "metadata": {},
   "source": [
    "# Code for the Crawler Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c456de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_between_keywords(text, start_keyword, end_keyword):\n",
    "    \"\"\"\n",
    "    Extracts and returns the text between two specified keywords in a given string.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The string from which to extract the text.\n",
    "    start_keyword (str): The keyword marking the beginning of the desired text.\n",
    "    end_keyword (str): The keyword marking the end of the desired text.\n",
    "\n",
    "    Returns:\n",
    "    str: The text found between the start_keyword and end_keyword. If start_keyword is not found, \n",
    "         an empty string is returned. If end_keyword is not found, the text from start_keyword \n",
    "         to the end of the string is returned.\n",
    "    \"\"\"\n",
    "    start_index = text.find(start_keyword)\n",
    "    if start_index == -1:\n",
    "        return \"\"\n",
    "    end_index = text.find(end_keyword, start_index)\n",
    "    if end_index != -1:\n",
    "        return text[start_index + len(start_keyword):end_index].strip()\n",
    "    else:\n",
    "        return text[start_index + len(start_keyword):].strip()\n",
    "\n",
    "def process_pdf_files(pdf_folder):\n",
    "    \"\"\"\n",
    "    Processes all PDF files in a specified folder, extracting text content based on predefined keyword pairs.\n",
    "\n",
    "    The function iterates through each PDF file in the given folder, extracts text between specified keywords,\n",
    "    and stores the results in a DataFrame. It also keeps track of any files that encountered errors during \n",
    "    processing and returns both the cleaned DataFrame and the list of error files.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_folder (str): The path to the folder containing the PDF files to be processed.\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - pd.DataFrame: A DataFrame containing the location of the PDF files and the extracted background text.\n",
    "                        Rows with missing values are dropped.\n",
    "        - list: A list of filenames that encountered errors during processing.\n",
    "    \"\"\"\n",
    "    data = []  # Initialize an empty list to store the processed data\n",
    "    error_files = []  # Initialize a list to keep track of files that cause errors\n",
    "    \n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            try:\n",
    "                with open(pdf_path, 'rb') as pdf_file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "                    num_pages = len(pdf_reader.pages)\n",
    "                    text_content = \"\"\n",
    "                    \n",
    "                    for page_num in range(num_pages):\n",
    "                        page = pdf_reader.pages[page_num]\n",
    "                        text_content += page.extract_text()\n",
    "                    \n",
    "                    # Extract background text based on predefined keyword pairs\n",
    "                    background_text_1 = extract_text_between_keywords(text_content, \"background\", \"my findings\")\n",
    "                    background_text_2 = extract_text_between_keywords(text_content, \"What happened\", \"What I’ve decided – and why\")\n",
    "                    \n",
    "                    # Use the first non-empty extraction result\n",
    "                    background_text = background_text_1 or background_text_2\n",
    "                    \n",
    "                    # Append the file's location and the extracted text to the data list\n",
    "                    data.append([f\"decision/{filename}\", background_text])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "                error_files.append(filename)\n",
    "                data.append([f\"decision/{filename}\", \"\"])  # Append an empty text entry for consistency\n",
    "    \n",
    "    # Create a DataFrame from the data list with specified column names\n",
    "    df = pd.DataFrame(data, columns=[\"location\", \"background\"])\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    df_cleaned = df.dropna()\n",
    "        \n",
    "    return df_cleaned, error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(text):\n",
    "    \"\"\"\n",
    "    Extract specific information from a given complaint text using the GPT-3.5-turbo model.\n",
    "\n",
    "    This function sends a structured prompt to the GPT-3.5-turbo model to extract the following information:\n",
    "    1. Gender of the complainant.\n",
    "    2. Category of the insurance product involved in the complaint.\n",
    "    3. Compensation amount mentioned in the complaint, or \"Non-pecuniary compensation\" if unspecified.\n",
    "    4. Reasons for the complaint related to:\n",
    "       - Service quality\n",
    "       - Misleading information\n",
    "       - Insurance policy management issues\n",
    "       - Delays in payment or processing\n",
    "       - Inadequate final compensation\n",
    "       - Claim denial\n",
    "       - Non-monetary services in the claims process\n",
    "\n",
    "    If the extracted reasons for complaints (questions 4 to 10) are all 0, the function will identify the most similar \n",
    "    reason from the causes of these questions and mark the corresponding reason as 1.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text of the complaint from which to extract the information.\n",
    "\n",
    "    Returns:\n",
    "    str: A JSON string containing the extracted information in the following format:\n",
    "         {\"Gender\": \"\", \"Category\": \"\", \"Compensation\": \"\", \"R1(Service quality)\": \"\", \"R2(Misleading information)\": \"\", \n",
    "         \"R3(Insurance policy management issues)\": \"\", \"R4(Delayed payment or processing)\": \"\", \"R5(Inadequate compensation)\": \"\", \n",
    "         \"R6(Claim denial)\": \"\", \"R7(Non-monetary services in claims)\": \"\"}.\n",
    "    \"\"\"\n",
    "    # Define the structured prompt for the GPT-3.5-turbo model to extract specific information\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an assistant that extracts specific information from complaints.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": (\n",
    "                f\"Extract the following information from the text:\\n\"\n",
    "                f\"1. Gender (must choose from: 0 for Male, 1 for Female, or 2 for Male and Female, can not return empty)\\n\"\n",
    "                f\"2. Category of insurance product (must choose one from: c1 for Life and Health Insurance, c2 for Medical Insurance, c3 for Personal Belongings Insurance, c4 for Property Insurance, c5 for Motor Insurance, c6 for Household Insurance, c7 for Pet Insurance, c8 for Travel Insurance, c9 for Landlord Insurance, c10 for PPI (Payment Protection Insurance), c11 for Combined and Other Insurance Types. The output must be a single choice (c1 to c11) and cannot be empty.)\\n\"\n",
    "                f\"3. Compensation amount (if not specified, return Non-pecuniary compensation)\\n\"\n",
    "                f\"4. The reason for the complaint is dissatisfaction with the service quality of the insurance company (if the complaint content includes this reason, output 1, if not included, count as 0)\\n\"\n",
    "                f\"5. The reason for the complaint is that the company provided misleading information during the insurance process (if the complaint content includes this reason, output 1; otherwise 0)\\n\"\n",
    "                f\"6. Complaints are due to poor policy management, including disputes over premiums, renewal issues, and policy cancellations (if the complaint includes this reason, output 1; otherwise 0)\\n\"\n",
    "                f\"7. Complaints are due to delays in payment or processing by the insurance company (if the complaint includes this reason, output 1; otherwise 0)\\n\"\n",
    "                f\"8. Complaints are due to inadequate final compensation by the insurance company (if the complaint includes this reason, output 1; otherwise 0)\\n\"\n",
    "                f\"9. Complaints are due to the insurance company ultimately refusing to pay the claim (if the complaint includes this reason, output 1; otherwise 0)\\n\"\n",
    "                f\"10. Complaints are due to dissatisfaction with non-monetary services in the insurance company's claims process (if the complaint includes this reason, output 1; otherwise 0)\\n\"\n",
    "                f\" If the answers from questions 4 to 10 are all 0, then identify the most similar reason from the causes of questions 4 through 10 and mark the corresponding question's answer as 1.\\n\"\n",
    "                f\"Text: {text}\\n\\n\"\n",
    "                f\"Provide the information in the following format:\\n\"\n",
    "                f'{{\"Gender\": \"\", \"Category\": \"\", \"Compensation\": \"\", \"R1(Service quality)\": \"\", \"R2(Misleading information)\": \"\", \"R3(Insurance policy management issues)\": \"\", \"R4(Delayed payment or processing)\": \"\", \"R5(Inadequate compensation)\": \"\", \"R6(Claim denial)\": \"\", \"R7(Non-monetary services in claims)\": \"\"}}'\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Send the structured prompt to the GPT-3.5-turbo model to generate the response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "        temperature=0,\n",
    "        top_p=0,\n",
    "    )\n",
    "    \n",
    "    # Return the extracted information from the model's response\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1fdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store final results\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# Set the current date for metadata extraction\n",
    "# Note: Adjust the anchor date backward by one day to avoid duplication\n",
    "current_date = datetime.datetime.strptime(\"2024-07-08\", \"%Y-%m-%d\")\n",
    "\n",
    "# Initialize a list to record files with errors during processing\n",
    "error_files = []\n",
    "\n",
    "# Begin the loop (currently set to iterate once for testing)\n",
    "# Adjust the range as needed\n",
    "for i in range(12*2+8):\n",
    "    print(f'Round {i}')\n",
    "    \n",
    "    ## Step 1: Scrape metadata\n",
    "    # Calculate the date 'i' iterations before the current date\n",
    "    date = current_date - timedelta(days=10*i)\n",
    "    # Convert the date to a string format suitable for the scrape command\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Run the scraping command to get metadata up to the calculated date\n",
    "    # Note: Replace \"your_script_path\" and \"your_target_directory\" with actual paths\n",
    "    !python your_script_path/scrape.py get-metadata --industry-sector \"payment-protection-insurance,insurance\" --to {date_str}\n",
    "    \n",
    "    # Check if the metadata file was successfully created and is not empty\n",
    "    metadata_file_path = 'your_target_directory/metadata.csv'\n",
    "    if not os.path.exists(metadata_file_path):\n",
    "        print(\"metadata.csv file doesn't exist, reentering the loop\")\n",
    "        continue\n",
    "    \n",
    "    # Load the metadata CSV into a DataFrame and select relevant columns\n",
    "    df_metadata = pd.read_csv(metadata_file_path)[['decision_id', 'date', 'company', 'decision']]\n",
    "    \n",
    "    # Map the 'decision' column to binary values: 'Upheld' = 1, 'Not upheld' = 0\n",
    "    df_metadata['decision'] = df_metadata['decision'].map({'Upheld': 1, 'Not upheld': 0})\n",
    "    \n",
    "    # If metadata is empty, skip to the next iteration\n",
    "    if df_metadata.empty:\n",
    "        print(\"metadata.csv file is empty, reentering the loop\")\n",
    "        continue\n",
    "    \n",
    "    ## Step 2: Extract Case IDs and related text content\n",
    "    # Run a command to download decision texts associated with the metadata\n",
    "    %run your_script_path/scrape.py download-decisions\n",
    "    \n",
    "    # Process the downloaded PDFs and extract text content\n",
    "    pdf_folder_path = \"your_target_directory/decisions\"\n",
    "    df_texts, errors = process_pdf_files(pdf_folder_path)\n",
    "    \n",
    "    # Log any errors encountered during the text extraction\n",
    "    if errors:\n",
    "        print(f\"Errors in round {i}: {errors}\")\n",
    "        error_files.append(f'df_texts_round_{i+1}.csv')\n",
    "    \n",
    "    ## Step 3: Extract structured information from text using an API\n",
    "    # Initialize a list to store JSON strings returned by the API\n",
    "    json_strings = []\n",
    "\n",
    "    # Process each text content in the extracted texts DataFrame\n",
    "    for text_content in df_texts['background']:\n",
    "        if text_content:  # Ensure the background text is not empty\n",
    "            extracted_info = extract_information(text_content)\n",
    "        else:\n",
    "            extracted_info = None\n",
    "\n",
    "        json_strings.append(extracted_info)\n",
    "\n",
    "    # Initialize lists to store extracted attributes\n",
    "    genders = []\n",
    "    categories = []\n",
    "    compensations = []\n",
    "    r1 = []\n",
    "    r2 = []\n",
    "    r3 = []\n",
    "    r4 = []\n",
    "    r5 = []\n",
    "    r6 = []\n",
    "    r7 = []\n",
    "\n",
    "    # Process each JSON string to extract and store individual data points\n",
    "    for json_str in json_strings:\n",
    "        if not json_str:\n",
    "            # If json_str is empty, fill with default values (None or 0)\n",
    "            genders.append(None)\n",
    "            categories.append(None)\n",
    "            compensations.append(None)\n",
    "            r1.append(0)\n",
    "            r2.append(0)\n",
    "            r3.append(0)\n",
    "            r4.append(0)\n",
    "            r5.append(0)\n",
    "            r6.append(0)\n",
    "            r7.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                # Load the JSON string and extract the data\n",
    "                data = json.loads(json_str)\n",
    "                genders.append(data.get(\"Gender\", None))\n",
    "                categories.append(data.get(\"Category\", None))\n",
    "                compensations.append(data.get(\"Compensation\", None))\n",
    "                r1.append(data.get(\"R1(Service quality)\", 0))\n",
    "                r2.append(data.get(\"R2(Misleading information)\", 0))\n",
    "                r3.append(data.get(\"R3(Insurance policy management issues)\", 0))\n",
    "                r4.append(data.get(\"R4(Delayed payment or processing)\", 0))\n",
    "                r5.append(data.get(\"R5(Inadequate compensation)\", 0))\n",
    "                r6.append(data.get(\"R6(Claim denial)\", 0))\n",
    "                r7.append(data.get(\"R7(Non-monetary services in claims)\", 0))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Invalid JSON string: {json_str}\")\n",
    "                # Handle invalid JSON by filling with default values\n",
    "                genders.append(None)\n",
    "                categories.append(None)\n",
    "                compensations.append(None)\n",
    "                r1.append(0)\n",
    "                r2.append(0)\n",
    "                r3.append(0)\n",
    "                r4.append(0)\n",
    "                r5.append(0)\n",
    "                r6.append(0)\n",
    "                r7.append(0)\n",
    "\n",
    "    # Create a DataFrame to store the extracted information\n",
    "    df_extracted = pd.DataFrame({\n",
    "        'location': df_texts['location'],\n",
    "        'gender': genders,\n",
    "        'category': categories,\n",
    "        'compensation': compensations,\n",
    "        'reason_1': r1,\n",
    "        'reason_2': r2,\n",
    "        'reason_3': r3,\n",
    "        'reason_4': r4,\n",
    "        'reason_5': r5,\n",
    "        'reason_6': r6,\n",
    "        'reason_7': r7,\n",
    "        'background': df_texts['background']\n",
    "    })\n",
    "    \n",
    "    ## Step 4: Merge metadata with the extracted features\n",
    "    df_temp = pd.merge(df_metadata, df_extracted, how='left', on='location')\n",
    "    df_final = pd.concat([df_final, df_temp], axis=0)\n",
    "    \n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    df_final.to_csv('your_target_directory/df_final.csv', index=False)\n",
    "    \n",
    "    # Save each iteration's data to a separate CSV file\n",
    "    output_file_name = f'your_target_directory/df_texts_round_{i+1}.csv'\n",
    "    df_temp.to_csv(output_file_name, index=False)\n",
    "    print(f\"Saved {output_file_name}\")\n",
    "\n",
    "    ## Step 5: Clean up temporary files to save space\n",
    "    # Check if the folder containing the decision texts exists\n",
    "    folder_path = \"your_target_directory/decisions\"\n",
    "    if os.path.exists(folder_path):\n",
    "        # If it exists, delete the folder and its contents\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Folder '{folder_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' does not exist.\")   \n",
    "\n",
    "    # Delete the metadata CSV file after processing\n",
    "    if os.path.exists(metadata_file_path):\n",
    "        os.remove(metadata_file_path)\n",
    "        print(f\"File '{metadata_file_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File '{metadata_file_path}' does not exist.\")\n",
    "        \n",
    "# Print all files that encountered errors during processing\n",
    "if error_files:\n",
    "    print(\"The following rounds had errors:\")\n",
    "    for file in error_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No errors occurred during processing.\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
